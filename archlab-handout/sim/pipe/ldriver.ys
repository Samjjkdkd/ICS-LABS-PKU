#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
# 谢昱辉2200017753
# Describe how and why you modified the baseline code.
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
# 优化选择：
# 1）在hcl文件中实现了加载传送，使得加载/使用冒险不再影响程序运行
# 2）选择10x1循环展开，提高程序在处理长数组时的运行效率。
# 3）对于小于10的部分，使用三叉树的结构，最大程度地优化运行。
# 4）在hcl中实现了iaddq指令，简化了程序中的部分指令。
	# Loop header
    iaddq $-10,%rdx 
    jl ROOT

L1: 
    mrmovq (%rdi), %r8
    rmmovq %r8,(%rsi)
    andq %r8, %r8
    jle L2
    iaddq $1,%rax    
L2: 
    mrmovq 8(%rdi),%r8
    rmmovq %r8,8(%rsi)
    andq %r8,%r8
    jle L3
    iaddq $1,%rax
L3:
    mrmovq 16(%rdi), %r8
    rmmovq %r8,16(%rsi)
    andq %r8, %r8
    jle L4
    iaddq $1, %rax      
L4:
    mrmovq 24(%rdi), %r8
    rmmovq %r8,24(%rsi)
    andq %r8,%r8
    jle L5
    iaddq $1,%rax
L5:
    mrmovq 32(%rdi), %r8
    rmmovq %r8,32(%rsi)
    andq %r8, %r8
    jle L6
    iaddq $1, %rax      
L6:
    mrmovq 40(%rdi), %r8
    rmmovq %r8,40(%rsi)
    andq %r8,%r8
    jle L7
    iaddq $1,%rax 
L7:
    mrmovq 48(%rdi), %r8
    rmmovq %r8,48(%rsi)
    andq %r8, %r8
    jle L8
    iaddq $1, %rax      
L8:
    mrmovq 56(%rdi), %r8
    rmmovq %r8,56(%rsi)
    andq %r8,%r8
    jle L9
    iaddq $1,%rax 
L9:
    mrmovq 64(%rdi), %r8
    rmmovq %r8,64(%rsi)
    andq %r8, %r8
    jle L10
    iaddq $1, %rax        
L10:
    mrmovq 72(%rdi), %r8
    rmmovq %r8,72(%rsi)
    andq %r8,%r8
    jle L11
    iaddq $1,%rax
L11:
    iaddq $80,%rdi
    iaddq $80,%rsi
    iaddq $-10,%rdx
    jge L1

ROOT:
    iaddq $7,%rdx
    jl  LEFT3        # <3
    jg  RIGHT3       # >3
    je  LEAF3        # =3

LEFT3:
    iaddq $2,%rdx       # len == 1
    je  LEAF1
    iaddq $-1,%rdx      # len == 2
    je  LEAF2
    ret                 # len == 0 

RIGHT3:
    iaddq $-3,%rdx
    jg  RIGHT6
    je  LEAF6           # len == 6
    iaddq $1,%rdx 
    jl  LEAF4           # len == 4
    je  LEAF5           # len == 5  
      
RIGHT6:
    iaddq $-2,%rdx
    jl  LEAF7           # len == 7
    je  LEAF8           # len == 8

LEAF9:
    mrmovq 64(%rdi), %r8
    rmmovq %r8, 64(%rsi)
    andq %r8, %r8
    jle LEAF8    
    iaddq $1, %rax        

LEAF8:
    mrmovq 56(%rdi), %r8   
    rmmovq %r8, 56(%rsi)
    andq %r8, %r8
    jle LEAF7   
    iaddq $1, %rax

LEAF7:
    mrmovq 48(%rdi), %r8
    rmmovq %r8, 48(%rsi)
    andq %r8, %r8
    jle LEAF6  
    iaddq $1, %rax   

LEAF6:
    mrmovq 40(%rdi), %r8
    rmmovq %r8, 40(%rsi)
    andq %r8, %r8
    jle LEAF5
    iaddq $1, %rax     

LEAF5:
    mrmovq 32(%rdi), %r8
    rmmovq %r8, 32(%rsi)
    andq %r8, %r8
    jle LEAF4  
    iaddq $1, %rax

LEAF4:
    mrmovq 24(%rdi), %r8
    rmmovq %r8, 24(%rsi)
    andq %r8, %r8
    jle LEAF3 
    iaddq $1, %rax

LEAF3:
    mrmovq 16(%rdi), %r8
    rmmovq %r8, 16(%rsi)
    andq %r8, %r8
    jle LEAF2   
    iaddq $1, %rax

LEAF2:
    mrmovq 8(%rdi), %r8
    rmmovq %r8, 8(%rsi)
    andq %r8, %r8 
    jle LEAF1  
    iaddq $1, %rax

LEAF1:
    mrmovq (%rdi), %r8
    rmmovq %r8, (%rsi)
    andq %r8, %r8
    jle Done 
    iaddq $1, %rax
##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad -2
	.quad 3
	.quad 4
	.quad -5
	.quad -6
	.quad 7
	.quad 8
	.quad -9
	.quad 10
	.quad -11
	.quad 12
	.quad 13
	.quad -14
	.quad -15
	.quad 16
	.quad -17
	.quad -18
	.quad 19
	.quad -20
	.quad 21
	.quad -22
	.quad -23
	.quad -24
	.quad 25
	.quad -26
	.quad 27
	.quad 28
	.quad -29
	.quad -30
	.quad 31
	.quad 32
	.quad 33
	.quad -34
	.quad -35
	.quad 36
	.quad -37
	.quad -38
	.quad 39
	.quad -40
	.quad -41
	.quad -42
	.quad 43
	.quad 44
	.quad 45
	.quad 46
	.quad 47
	.quad -48
	.quad 49
	.quad -50
	.quad 51
	.quad -52
	.quad 53
	.quad 54
	.quad -55
	.quad -56
	.quad -57
	.quad -58
	.quad -59
	.quad -60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
